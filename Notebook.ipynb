{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducibility Project: Readmission Prediction via Deep Contextual Embedding of Clinical Concepts\n",
    "\n",
    "## Reproducibility summary\n",
    "\n",
    "This project reproduces the CONTENT model, a deep learning model predicting hospital readmissions using interpretable patient representations from Electronic Health Records (EHR). This project independently implements the model using PyTorch due to missing source code and outdated Python libraries in the original paper. It compares the performance of the implemented CONTENT model with the RNN with Gated Recurrent Unit (GRU) model and the results from the original paper.\n",
    "\n",
    "The CONTENT model, which combines topic modeling and Recurrent Neural Network (RNN), outperformed the GRU model as claimed in the original paper. Our implementation on CONTENT has also shown better performance than the RNN with GRU unit model consistently. The LSTM CONTENT model had lower accuracy and longer training time compared to the GRU model, indicating that GRU is better suited for the dataset. Our CONTENT implementation also has an accuracy of 11.82% higher, ROC-AUC score 16.1481% higher, and PR-AUC score 8.1496% higher than the original paper.\n",
    "\n",
    "Implementation discrepancies could be due to the differences in hyperparameter and the differences in the implementation of missing functions and files. The incomplete source code complicates identifying the source of the performance discrepancy between the our own PyTorch implementation and the original Lasagne-based model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import DataProcessing as dp\n",
    "from Hyperpara_config import Config\n",
    "from PatientDataLoader import Data_Loader\n",
    "import PyTorch_CONTENT\n",
    "\n",
    "RAW_DATA_PATH = \"./S1_File.txt\"\n",
    "RAW_DATA_SORTED_PATH = \"./raw_data_sorted.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(isSorted = False):\n",
    "    # Read in the data from the input file\n",
    "    data = pd.read_csv(RAW_DATA_PATH, sep=\"\\t\", header=0)\n",
    "    data.to_csv(RAW_DATA_SORTED_PATH, sep=\"\\t\", index=False)\n",
    "    return data\n",
    "\n",
    "data = get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of data statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of row data:\n",
      "   PID  DAY_ID                               DX_GROUP_DESCRIPTION  \\\n",
      "0    1   73888                                    ANGINA PECTORIS   \n",
      "1    1   73888  MONONEURITIS OF UPPER LIMB AND MONONEURITIS MU...   \n",
      "2    1   73888  SYMPTOMS INVOLVING RESPIRATORY SYSTEM AND OTHE...   \n",
      "3    1   73880                                 ACUTE APPENDICITIS   \n",
      "4    1   73880                                  DIABETES MELLITUS   \n",
      "\n",
      "     SERVICE_LOCATION  OP_DATE  \n",
      "0      DOCTORS OFFICE    74084  \n",
      "1      DOCTORS OFFICE    74084  \n",
      "2      DOCTORS OFFICE    74084  \n",
      "3  INPATIENT HOSPITAL    74084  \n",
      "4  INPATIENT HOSPITAL    74084  \n",
      "\n",
      "Column names: PID, DAY_ID, DX_GROUP_DESCRIPTION, SERVICE_LOCATION, OP_DATE\n",
      "Number of rows(events): 685482\n",
      "Number of columnss: 5\n",
      "Number of patients: 3000\n",
      "Number of different procedure description: 1412\n",
      "Number of admissions: 30742\n",
      "Average number of events per patient: 228.494\n"
     ]
    }
   ],
   "source": [
    "print(\"First 5 rows of row data:\")\n",
    "print(data.head())\n",
    "print()\n",
    "\n",
    "columns = list(data.columns)\n",
    "print(\"Column names:\", \", \".join(columns))\n",
    "print(f\"Number of rows(events): {data.shape[0]}\")\n",
    "print(f\"Number of columnss: {data.shape[1]}\")\n",
    "print(f\"Number of patients: {data['PID'].nunique()}\")\n",
    "print(f\"Number of different procedure description: {data['DX_GROUP_DESCRIPTION'].nunique()}\")\n",
    "inpatient_events = data[data['SERVICE_LOCATION'] == \"INPATIENT HOSPITAL\"]\n",
    "num_admission = len(inpatient_events.groupby(['PID', 'DAY_ID']))\n",
    "print(f\"Number of admissions: {num_admission}\")\n",
    "print(f\"Average number of events per patient: {data.shape[0]/data['PID'].nunique()}\")\n",
    "# num_combinations = len(data.groupby(['PID', 'DAY_ID', 'SERVICE_LOCATION']))\n",
    "# print(\"Number of unique combinations:\", num_combinations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "In this reproducibility study, we employed the same approach as the original paper to assess the effectiveness of the CONTENT model. However, we independently implemented the CONTENT model using the model's description in the paper due to the missing source code and outdated Python libraries. Next, we compared the performance of our implementation with the reported results in the original paper. Finally, we compared the performance of the CONTENT model with that of the GRU model to determine whether the CONTENT model outperforms the RNN with GRU model, as claimed in the original paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block will process the data, train and test the CONTENT model that we implemented as an example. The whole experiments ran each models 10 times to obtain the results. We are running the CONTENT model once just to demonstrate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------Loading and processing data------------\n",
      "Loading and sorting raw data......\n",
      "Creating Stop and Vocab files......\n",
      "Mapping vocab to index......\n",
      "Extracting in patient events......\n",
      "Processing data to sequence and labels......\n",
      "Splitting data into training, testing and validation sets......\n",
      "Total data processing time: 169.391s\n",
      "------------Complete loading and processing data------------\n",
      "------CONTENT model------\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Documents\\School\\UIUC\\Spring 2023\\CS Deep Learning for Healthcare\\Project\\PyTorch_CONTENT.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  yield [torch.tensor(input[excerpt]) for input in inputs]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 \t\t Training Loss: 84.53390168952942\n",
      "Epoch 1 \t\t Validation Loss: 84.16148558934529\n",
      "Validation Loss Decreased(inf--->50496.891354) \t Saving The Model\n",
      "Epoch 1 of 6 took 452.805s\n",
      "Epoch 2 \t\t Training Loss: 82.91657370185852\n",
      "Epoch 2 \t\t Validation Loss: 82.98400868733724\n",
      "Validation Loss Decreased(50496.891354--->49790.405212) \t Saving The Model\n",
      "Epoch 2 of 6 took 465.414s\n",
      "Epoch 3 \t\t Training Loss: 82.47320268440247\n",
      "Epoch 3 \t\t Validation Loss: 83.29619951883952\n",
      "Epoch 3 of 6 took 464.233s\n",
      "Epoch 4 \t\t Training Loss: 82.01510676765442\n",
      "Epoch 4 \t\t Validation Loss: 82.8860942586263\n",
      "Validation Loss Decreased(49790.405212--->49731.656555) \t Saving The Model\n",
      "Epoch 4 of 6 took 455.043s\n",
      "Epoch 5 \t\t Training Loss: 81.7331255569458\n",
      "Epoch 5 \t\t Validation Loss: 83.05654661814371\n",
      "Epoch 5 of 6 took 451.772s\n",
      "Epoch 6 \t\t Training Loss: 81.27637366485595\n",
      "Epoch 6 \t\t Validation Loss: 83.02824993769327\n",
      "Epoch 6 of 6 took 497.585s\n",
      "Total time to train: 2787.6416029930115\n",
      "Testing...\n",
      "Test roc_auc:\t\t0.801897\n",
      "Test pr_auc:\t\t0.653389\n",
      "Test acc:\t\t0.834858\n",
      "Total time to test: 34.39737582206726\n",
      "Total Memory Used in Training, validating and testing: 1096944.0 MB\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "def DataProcessing(isSorted = False):\n",
    "    print(\"------------Loading and processing data------------\")\n",
    "    start_time = time.time()\n",
    "    print(\"Loading and sorting raw data......\")\n",
    "    data = dp.get_data(isSorted) # load the raw data\n",
    "    print(\"Creating Stop and Vocab files......\")\n",
    "    dp.stop_vocab_generation(data) # Writes vocab.txt and stop.txt, using data. vocab.txt contains all the description that has appearance more than x\n",
    "    print(\"Mapping vocab to index......\")\n",
    "    word_index_dict = dp.load_vocab_index_dict() # Save a vocab.pkl file, it's a dict\n",
    "    print(\"Extracting in patient events......\")\n",
    "    eventsDF = dp.extract_inpatient_events() # Extracts all the inpatient events, it's a dataframe\n",
    "    print(\"Processing data to sequence and labels......\")\n",
    "    seq, labels = dp.seq_label_gen(word_index_dict, eventsDF) # create the sequence and labels for training\n",
    "    print(\"Splitting data into training, testing and validation sets......\")\n",
    "    dp.splits(seq, labels) # splits the training, validation and testing sets\n",
    "    print(\"Total data processing time: {:.3f}s\".format(time.time() - start_time))\n",
    "    print(\"------------Complete loading and processing data------------\")\n",
    "\n",
    "DataProcessing()\n",
    "FLAGS = Config()\n",
    "data_set = Data_Loader(FLAGS)\n",
    "iterator = data_set.iterator()\n",
    "isRNN = False\n",
    "isCONTENT = True\n",
    "isLSTM = False\n",
    "PyTorch_CONTENT.run(data_set, FLAGS, isCONTENT, isRNN, isLSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results of 10 runs of each models:\n",
    "\n",
    "| Model | ROC-AUC | PR-AUC | ACC |\n",
    "|-------|---------|--------|-----|\n",
    "| CONTENT (PyTorch, own implementation) | 0.7998±0.0014 | 0.6501±0.0018 | 0.8352±0.0001 |\n",
    "| CONTENT (reported in paper) | 0.6886±0.0074 | 0.6011±0.0191 | 0.7170±0.0069 |\n",
    "| GRU (reported in paper) | 0.6881±0.0048 | 0.5929±0.0100 | 0.7141±0.0040 |\n",
    "| GRU (own implementation) | 0.7937±0.0003 | 0.6445±0.0012 | 0.8318±0.0017 |\n",
    "| CONTENT w/ LSTM (own implementation) | 0.7937±0.0024 | 0.6440±0.0003 | 0.8320±0.0010 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "Cao Xiao, Tengfei Ma, Adji B. Dieng, David M. Blei, and Fei Wang. 2018. [Readmission prediction via deep contextual embedding of clinical concepts](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0195024). PLOS ONE, 13:e0195024.\n",
    "\n",
    "Cao Xiao, Tengfei Ma, Adji B. Dieng, David M. Blei, and Fei Wang. (2018). CONTENT (Version 1.0.0) [Computer software].https://doi.org/10.1371/journal.pone.0195024"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
